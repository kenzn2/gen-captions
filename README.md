# Vietnamese Image Captioning with Flickr8k

<div align="center">
  <img src="https://img.shields.io/badge/Deep_Learning-Image_Captioning-blue?style=for-the-badge&logo=python">
  <img src="https://img.shields.io/badge/Language-Vietnamese-green?style=for-the-badge">
  <img src="https://img.shields.io/badge/Framework-TensorFlow_Keras-orange?style=for-the-badge">
  <img src="https://img.shields.io/badge/Dataset-Flickr8k-red?style=for-the-badge">
</div>

## ğŸ“‹ Tá»•ng Quan (Overview)

Dá»± Ã¡n nÃ y triá»ƒn khai há»‡ thá»‘ng tá»± Ä‘á»™ng táº¡o mÃ´ táº£ tiáº¿ng Viá»‡t cho hÃ¬nh áº£nh sá»­ dá»¥ng Deep Learning. MÃ´ hÃ¬nh Ä‘Æ°á»£c xÃ¢y dá»±ng dá»±a trÃªn kiáº¿n trÃºc Encoder-Decoder vá»›i CNN (InceptionV3) Ä‘á»ƒ trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng hÃ¬nh áº£nh vÃ  LSTM Ä‘á»ƒ sinh vÄƒn báº£n tiáº¿ng Viá»‡t.

This project implements an automatic Vietnamese image captioning system using Deep Learning. The model is built based on an Encoder-Decoder architecture with CNN (InceptionV3) for image feature extraction and LSTM for Vietnamese text generation.

## ğŸ¯ TÃ­nh NÄƒng ChÃ­nh (Key Features)

- **Táº¡o Caption Tiáº¿ng Viá»‡t**: Tá»± Ä‘á»™ng sinh mÃ´ táº£ tiáº¿ng Viá»‡t cho hÃ¬nh áº£nh
- **CNN + LSTM Architecture**: Káº¿t há»£p InceptionV3 vÃ  Bidirectional LSTM
- **Word Embedding**: Sá»­ dá»¥ng FastText Vietnamese word vectors
- **Text Processing**: Xá»­ lÃ½ vÄƒn báº£n tiáº¿ng Viá»‡t vá»›i underthesea
- **Attention Mechanism**: CÆ¡ cháº¿ attention Ä‘á»ƒ cáº£i thiá»‡n cháº¥t lÆ°á»£ng caption

## ğŸ—ï¸ Kiáº¿n TrÃºc MÃ´ HÃ¬nh (Model Architecture)

### 1. Image Encoder
- **Model**: InceptionV3 pre-trained trÃªn ImageNet
- **Input**: HÃ¬nh áº£nh 299x299 pixels
- **Output**: Feature vector 2048 chiá»u
- **Preprocessing**: Chuáº©n hÃ³a theo InceptionV3

### 2. Text Decoder  
- **Embedding**: FastText Vietnamese (300 dimensions)
- **LSTM**: Bidirectional LSTM layers
- **Attention**: Add attention mechanism
- **Output**: Softmax over vocabulary

### 3. Training Configuration
```python
EPOCHS = 100
BATCH_SIZE = 32
OPTIMIZER = Adam
VOCABULARY_SIZE = 3299
MAX_SEQUENCE_LENGTH = 41
EMBEDDING_DIM = 300
```

## ğŸ“Š Dataset Information

### Flickr8k Vietnamese Captions
- **Total Images**: 8,091 hÃ¬nh áº£nh
- **Captions per Image**: 5 mÃ´ táº£ tiáº¿ng Viá»‡t
- **Vocabulary Size**: 6,851 tá»« unique (cleaned: 3,299)
- **Train/Test Split**: Theo file trainImages.txt vÃ  testImages.txt

### Data Processing Pipeline
1. **Text Cleaning**: Loáº¡i bá» dáº¥u cÃ¢u, chuáº©n hÃ³a vÄƒn báº£n
2. **Tokenization**: Sá»­ dá»¥ng underthesea word_tokenize
3. **Sequence Formatting**: `startseq [caption] endseq`
4. **Padding**: Padding sequences Ä‘áº¿n max_length

## ğŸ› ï¸ CÃ´ng Nghá»‡ Sá»­ Dá»¥ng (Technology Stack)

### Core Libraries
- **TensorFlow/Keras**: Deep learning framework
- **InceptionV3**: Pre-trained CNN model
- **FastText**: Vietnamese word embeddings
- **underthesea**: Vietnamese NLP toolkit

### Supporting Tools
- **NumPy/Pandas**: Data manipulation
- **Matplotlib**: Visualization
- **PIL/OpenCV**: Image processing
- **NLTK**: Text evaluation (BLEU score)
- **tqdm**: Progress tracking

## âš™ï¸ CÃ i Äáº·t vÃ  Sá»­ Dá»¥ng (Installation & Usage)

### 1. Dependencies Installation

```bash
pip install tensorflow keras pillow numpy tqdm
pip install underthesea gensim fasttext nltk
pip install matplotlib opencv-python
```

### 2. Dataset Preparation

```python
# Download required datasets
# - Flickr8k Images
# - Vietnamese Captions (captions_vi.txt)
# - FastText Vietnamese vectors (cc.vi.300.bin)
```

### 3. Feature Extraction

```python
from keras.applications.inception_v3 import InceptionV3, preprocess_input

# Extract image features
model = InceptionV3(weights='imagenet')
model = Model(model.input, model.layers[-2].output)

# Process images
for img in images:
    image = load_img(img, target_size=(299, 299))
    image = preprocess_input(image)
    feature = model.predict(image)
```

### 4. Model Training

```python
# Build model
def define_model(vocab_size, max_length, embedding_matrix):
    # Image feature input
    inputs1 = Input(shape=(2048,))
    fe1 = Dropout(0.5)(inputs1)
    fe2 = Dense(256, activation='relu')(fe1)
    
    # Sequence input
    inputs2 = Input(shape=(max_length,))
    se1 = Embedding(vocab_size, 300, weights=[embedding_matrix])(inputs2)
    se2 = Dropout(0.5)(se1)
    se3 = Bidirectional(LSTM(256))(se2)
    
    # Combine and output
    decoder1 = add([fe2, se3])
    decoder2 = Dense(256, activation='relu')(decoder1)
    outputs = Dense(vocab_size, activation='softmax')(decoder2)
    
    model = Model(inputs=[inputs1, inputs2], outputs=outputs)
    return model
```

### 5. Caption Generation

```python
def generate_caption(image_path):
    # Extract features
    feature = extract_features(image_path)
    
    # Generate sequence
    caption = 'startseq'
    for i in range(max_length):
        sequence = tokenizer.texts_to_sequences([caption])[0]
        sequence = pad_sequences([sequence], maxlen=max_length)
        
        prediction = model.predict([feature, sequence])
        predicted = np.argmax(prediction)
        word = tokenizer.index_word[predicted]
        
        if word == 'endseq':
            break
        caption += ' ' + word
    
    return caption
```

## ğŸ“ˆ Káº¿t Quáº£ vÃ  ÄÃ¡nh GiÃ¡ (Results & Evaluation)

### Training Metrics
- **Loss Function**: Categorical Crossentropy
- **Training Time**: ~3-4 hours on Tesla T4 GPU
- **Convergence**: Model converges after ~50-60 epochs
- **Memory Usage**: ~8GB GPU memory

### Evaluation Metrics
- **BLEU Score**: ÄÃ¡nh giÃ¡ cháº¥t lÆ°á»£ng caption
- **Perplexity**: Äo Ä‘á»™ phá»©c táº¡p cá»§a mÃ´ hÃ¬nh ngÃ´n ngá»¯
- **Human Evaluation**: ÄÃ¡nh giÃ¡ chá»§ quan vá» tÃ­nh tá»± nhiÃªn

### Sample Outputs
```
Input: [HÃ¬nh áº£nh má»™t Ä‘á»©a tráº» Ä‘ang chÆ¡i]
Output: "má»™t Ä‘á»©a tráº» Ä‘ang chÆ¡i trong cÃ´ng viÃªn vá»›i quáº£ bÃ³ng"

Input: [HÃ¬nh áº£nh chÃ³ Ä‘ang cháº¡y]  
Output: "má»™t con chÃ³ Ä‘ang cháº¡y trÃªn bÃ£i cá» xanh"
```

## ğŸ”§ Tá»‘i Æ¯u HÃ³a vÃ  Cáº£i Tiáº¿n (Optimization & Improvements)

### Current Optimizations
- **Data Augmentation**: Rotation, flip, brightness adjustment
- **Dropout Regularization**: Prevent overfitting
- **Bidirectional LSTM**: Better sequence understanding
- **Pre-trained Embeddings**: FastText Vietnamese vectors

### Future Improvements
- **Attention Mechanism**: Visual attention for better focus
- **Transformer Architecture**: Use BERT/GPT-style models
- **Beam Search**: Better decoding strategy
- **Multi-modal Features**: Combine multiple feature extractors

## ğŸ“ Cáº¥u TrÃºc Dá»± Ãn (Project Structure)

```
gen-captions/
â”œâ”€â”€ flickr8k-vietnamese-captions.ipynb    # Main notebook
â”œâ”€â”€ README.md                             # Project documentation
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ captions_vietnamese_flickr8k.h5  # Trained model
â”‚   â”œâ”€â”€ tokenizer.pkl                    # Text tokenizer
â”‚   â””â”€â”€ features_inception_v3.pkl        # Image features
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ captions_vi.txt                  # Vietnamese captions
â”‚   â”œâ”€â”€ trainImages.txt                  # Training image list
â”‚   â””â”€â”€ testImages.txt                   # Test image list
â””â”€â”€ utils/
    â”œâ”€â”€ preprocessing.py                  # Data preprocessing
    â”œâ”€â”€ model_utils.py                   # Model utilities
    â””â”€â”€ evaluation.py                    # Evaluation metrics
```

## ğŸ” Chi Tiáº¿t Ká»¹ Thuáº­t (Technical Details)

### Text Processing
```python
# Vietnamese text cleaning
def cleaning_text(descriptions):
    table = str.maketrans('', '', string.punctuation)
    for img, captions in descriptions.items():
        for idx, cap in enumerate(captions):
            desc = cap.replace(' - ', ' ').translate(table)
            desc = ' '.join(simple_preprocess(desc))
            desc = word_tokenize(desc, format='text')
            descriptions[img][idx] = desc
    return descriptions
```

### Feature Extraction
```python
# Image feature extraction with InceptionV3
def extract_feature(directory):
    model = InceptionV3(weights='imagenet')
    model = Model(model.input, model.layers[-2].output)
    
    features = dict()
    for img in tqdm(os.listdir(directory)):
        filename = directory + '/' + img
        image = load_img(filename, target_size=(299, 299))
        image = img_to_array(image)
        image = np.expand_dims(image, axis=0)
        image = preprocess_input(image)
        
        feature = model.predict(image, verbose=0)
        features[img] = feature
    return features
```

## ğŸ“š TÃ i Liá»‡u Tham Kháº£o (References)

- [Show and Tell: Neural Image Caption Generation](https://arxiv.org/abs/1411.4555)
- [Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/abs/1502.03044)
- [Flickr8k Dataset](https://www.kaggle.com/datasets/adityajn105/flickr8k)
- [FastText Vietnamese Word Vectors](https://fasttext.cc/docs/en/crawl-vectors.html)
- [underthesea - Vietnamese NLP](https://github.com/undertheseanlp/underthesea)

## ğŸ¤ ÄÃ³ng GÃ³p (Contributing)

1. Fork repository
2. Táº¡o feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit changes (`git commit -m 'Add AmazingFeature'`)
4. Push to branch (`git push origin feature/AmazingFeature`)
5. Táº¡o Pull Request

## ğŸ“„ License

Dá»± Ã¡n nÃ y Ä‘Æ°á»£c phÃ¢n phá»‘i dÆ°á»›i MIT License. Xem file `LICENSE` Ä‘á»ƒ biáº¿t thÃªm chi tiáº¿t.

## ğŸ“ LiÃªn Há»‡ (Contact)

- **Author**: kenzn2
- **GitHub**: [@kenzn2](https://github.com/kenzn2)
- **Project**: [gen-captions](https://github.com/kenzn2/gen-captions)

## ğŸ™ Acknowledgments

- Flickr8k dataset creators
- FastText team for Vietnamese word vectors
- underthesea project for Vietnamese NLP tools
- TensorFlow/Keras development team
- Kaggle community for computational resources

---

<div align="center">
  <b>ğŸ–¼ï¸ Biáº¿n hÃ¬nh áº£nh thÃ nh lá»i nÃ³i tiáº¿ng Viá»‡t! ğŸ“</b>
</div>